

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lib.agents package &mdash; src  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lib.argParsers package" href="lib.argParsers.html" />
    <link rel="prev" title="lib package" href="lib.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> src
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="RLalgos.html">RLalgos module</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="lib.html">lib package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lib.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">lib.agents package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.Agent_DQN">lib.agents.Agent_DQN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.Agent_DoubleDQN">lib.agents.Agent_DoubleDQN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.policy">lib.agents.policy module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.qNetwork">lib.agents.qNetwork module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.randomActor">lib.agents.randomActor module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.sequentialActor">lib.agents.sequentialActor module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.sequentialCritic">lib.agents.sequentialCritic module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents.trainAgents">lib.agents.trainAgents module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-lib.agents">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lib.argParsers.html">lib.argParsers package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib.databaseIO.html">lib.databaseIO package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib.envs.html">lib.envs package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib.resultGraph.html">lib.resultGraph package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib.rl.html">lib.rl package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib.testLib.html">lib.testLib package</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib.utils.html">lib.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lib.html#module-lib">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="logs.html">logs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">modules package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">src</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="lib.html">lib package</a> &raquo;</li>
        
      <li>lib.agents package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/lib.agents.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="lib-agents-package">
<h1>lib.agents package<a class="headerlink" href="#lib-agents-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-lib.agents.Agent_DQN">
<span id="lib-agents-agent-dqn-module"></span><h2>lib.agents.Agent_DQN module<a class="headerlink" href="#module-lib.agents.Agent_DQN" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="lib.agents.Agent_DQN.Agent_DQN">
<em class="property">class </em><code class="descclassname">lib.agents.Agent_DQN.</code><code class="descname">Agent_DQN</code><span class="sig-paren">(</span><em>env</em>, <em>memory</em>, <em>qNetworkSlow</em>, <em>qNetworkFast</em>, <em>numActions</em>, <em>gamma</em>, <em>device='cpu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.checkTrainingMode">
<code class="descname">checkTrainingMode</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.checkTrainingMode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.checkTrainingMode" title="Permalink to this definition">¶</a></dt>
<dd><p>[summary]</p>
<p>[description]</p>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.epsGreedyAction">
<code class="descname">epsGreedyAction</code><span class="sig-paren">(</span><em>state</em>, <em>eps=0.999</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.epsGreedyAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.epsGreedyAction" title="Permalink to this definition">¶</a></dt>
<dd><p>epsilon greedy action</p>
<p>This is the epsilon greedy action. In general, this is going to
select the maximum action <code class="docutils literal notranslate"><span class="pre">eps</span></code> percentage of the times, while
selecting the random action the rest of the time. It is assumed
that this will receive a value of epsilon between 0 and 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>state</strong> (<em>{ndarray}</em>) – [description]</li>
<li><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Determines the fraction of times the max action will be selected
in comparison to a random action. (the default is 0.999)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The 1d tensor that has an action for each state provided.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.eval">
<code class="descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>[summary]</p>
<p>[description]</p>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.fastUpdate">
<code class="descname">fastUpdate</code><span class="sig-paren">(</span><em>tau=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.fastUpdate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.fastUpdate" title="Permalink to this definition">¶</a></dt>
<dd><p>update the fast network slightly</p>
<p>This is going to update the slow network slightly. The amount
is dictated by <code class="docutils literal notranslate"><span class="pre">tau</span></code>. This should be a number between 0 and 1.
It will update the <code class="docutils literal notranslate"><span class="pre">tau</span></code> fraction of the slow network weights
with the new weights. This is done for providing stability to the
network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tau</strong> (<em>{number}</em><em>, </em><em>optional</em>) – This parameter determines how much of the fast Networks weights
will be updated to the ne parameters weights (the default is 0.1)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>folder</em>, <em>name</em>, <em>map_location=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.load" title="Permalink to this definition">¶</a></dt>
<dd><p>load the model</p>
<p>An agent saved with the save command can be safely loaded with this command.
This will load both the qNetworks, as well as the memory buffer. There is a
possibility that one may not want to load the model into the same device. In
that case, the user should insert the device that the user wants to load the
model into.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>folder</strong> (<em>{str}</em>) – folder into which the model should be saved.</li>
<li><strong>name</strong> (<em>{str}</em>) – A name to associate the model to load. It is absolutelty possible to save
a number of models within the same folder, and hence the name can retrieve
that model that is important.</li>
<li><strong>map_location</strong> (<em>{str}</em><em>, </em><em>optional</em>) – The device in which to load the file. This is a string like ‘cpu’, ‘cuad:0’
etc. (the default is None, which results in the model being loaded to the
originam device)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.maxAction">
<code class="descname">maxAction</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.maxAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.maxAction" title="Permalink to this definition">¶</a></dt>
<dd><p>returns the action that maximizes the Q function</p>
<p>Given an set of statees, this function is going to return a set
of actions which will maximize the value of the Q network for each
of the supplied states.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>{nd_array</em><em> or </em><em>tensor}</em>) – numpy array or tensor containing the state. The columns
represent the different parts of the state.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The return values of actions that maximize the states</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">uarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.memoryUpdateEpisode">
<code class="descname">memoryUpdateEpisode</code><span class="sig-paren">(</span><em>policy</em>, <em>maxSteps=1000</em>, <em>minScoreToAdd=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.memoryUpdateEpisode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.memoryUpdateEpisode" title="Permalink to this definition">¶</a></dt>
<dd><p>update the memory</p>
<p>Given a particular policy, this memory is going to take
the policy and generate a series of memories and update
thememory buffer. Generating memories is easier to do
using this function than an external function …</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>policy</strong> (<em>{function}</em>) – This is a function that takes a state and returns an action. This
defines how the agent will explore the environment by changing the
exploration/exploitation scale.</li>
<li><strong>maxSteps</strong> (<em>{number}</em><em>, </em><em>optional</em>) – The maximum number of steps that one shoule have within an episode.
(the default is 1000)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.randomAction">
<code class="descname">randomAction</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.randomAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.randomAction" title="Permalink to this definition">¶</a></dt>
<dd><p>returns a set of random actions for the given states</p>
<p>given the size of the number of actions, this function is going
to return a set of actions that has the same number of actions
as the number of inputs in the shape. For example, if
<code class="docutils literal notranslate"><span class="pre">state.shape</span> <span class="pre">==</span> <span class="pre">(10,</span> <span class="pre">?)</span></code> then the result will be a vector of
size 10. This is in accordance with the redduction in the
dimensionality of the maxAction space.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>{nd_array</em><em> or </em><em>tensor}</em>) – numpy array or tensor containing the state. The columns
represent the different parts of the state.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The return value is set of random actions</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">uarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>folder</em>, <em>name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.save" title="Permalink to this definition">¶</a></dt>
<dd><p>save the model</p>
<p>This function allows one to save the model, in a folder that is
specified, with the fast and the slow qNetworks, as well as the
memory buffer. Sometimes there may be more than a single agent,
and under those circumstances, the name will come in handy. If the
supplied folder does not exist, it will be generated.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>folder</strong> (<em>{str}</em>) – folder into which the model should be saved.</li>
<li><strong>name</strong> (<em>{str}</em>) – A name to associate the current model with. It is
absolutelty possible to save a number of models within
the same folder.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.sigmaMaxAction">
<code class="descname">sigmaMaxAction</code><span class="sig-paren">(</span><em>state</em>, <em>sigma=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.sigmaMaxAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.sigmaMaxAction" title="Permalink to this definition">¶</a></dt>
<dd><p>returns the action that maximizes the noisy Q function</p>
<p>Given an set of statees, this function is going to return a set
of actions which will maximize the value of the Q network for each
of the supplied states, after adding Gaussian noise to the layers.
This is alternative to using an $epsilon$-greedy policy, and has
shown to provide better results under most circumstances.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>{nd_array</em><em> or </em><em>tensor}</em>) – numpy array or tensor containing the state. The columns
represent the different parts of the state.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The return values of actions that maximize the states</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">uarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.softUpdate">
<code class="descname">softUpdate</code><span class="sig-paren">(</span><em>tau=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.softUpdate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.softUpdate" title="Permalink to this definition">¶</a></dt>
<dd><p>update the slow network slightly</p>
<p>This is going to update the slow network slightly. The amount
is dictated by <code class="docutils literal notranslate"><span class="pre">tau</span></code>. This should be a number between 0 and 1.
It will update the <code class="docutils literal notranslate"><span class="pre">tau</span></code> fraction of the slow network weights
with the new weights. This is done for providing stability to the
network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tau</strong> (<em>{number}</em><em>, </em><em>optional</em>) – This parameter determines how much of the fast Networks weights
will be updated to the ne parameters weights (the default is 0.1)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DQN.Agent_DQN.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>nSamples=100</em>, <em>sigma=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DQN.html#Agent_DQN.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DQN.Agent_DQN.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-lib.agents.Agent_DoubleDQN">
<span id="lib-agents-agent-doubledqn-module"></span><h2>lib.agents.Agent_DoubleDQN module<a class="headerlink" href="#module-lib.agents.Agent_DoubleDQN" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN">
<em class="property">class </em><code class="descclassname">lib.agents.Agent_DoubleDQN.</code><code class="descname">Agent_DoubleDQN</code><span class="sig-paren">(</span><em>env</em>, <em>memory</em>, <em>qNetworkSlow</em>, <em>qNetworkFast</em>, <em>numActions</em>, <em>gamma</em>, <em>device='cpu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.checkTrainingMode">
<code class="descname">checkTrainingMode</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.checkTrainingMode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.checkTrainingMode" title="Permalink to this definition">¶</a></dt>
<dd><p>[summary]</p>
<p>[description]</p>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.epsGreedyAction">
<code class="descname">epsGreedyAction</code><span class="sig-paren">(</span><em>state</em>, <em>eps=0.999</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.epsGreedyAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.epsGreedyAction" title="Permalink to this definition">¶</a></dt>
<dd><p>epsilon greedy action</p>
<p>This is the epsilon greedy action. In general, this is going to
select the maximum action <code class="docutils literal notranslate"><span class="pre">eps</span></code> percentage of the times, while
selecting the random action the rest of the time. It is assumed
that this will receive a value of epsilon between 0 and 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>state</strong> (<em>{ndarray}</em>) – [description]</li>
<li><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Determines the fraction of times the max action will be selected
in comparison to a random action. (the default is 0.999)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The 1d tensor that has an action for each state provided.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.eval">
<code class="descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>[summary]</p>
<p>[description]</p>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.fastUpdate">
<code class="descname">fastUpdate</code><span class="sig-paren">(</span><em>tau=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.fastUpdate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.fastUpdate" title="Permalink to this definition">¶</a></dt>
<dd><p>update the fast network slightly</p>
<p>This is going to update the slow network slightly. The amount
is dictated by <code class="docutils literal notranslate"><span class="pre">tau</span></code>. This should be a number between 0 and 1.
It will update the <code class="docutils literal notranslate"><span class="pre">tau</span></code> fraction of the slow network weights
with the new weights. This is done for providing stability to the
network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tau</strong> (<em>{number}</em><em>, </em><em>optional</em>) – This parameter determines how much of the fast Networks weights
will be updated to the ne parameters weights (the default is 0.1)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>folder</em>, <em>name</em>, <em>map_location=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.load" title="Permalink to this definition">¶</a></dt>
<dd><p>load the model</p>
<p>An agent saved with the save command can be safely loaded with this command.
This will load both the qNetworks, as well as the memory buffer. There is a
possibility that one may not want to load the model into the same device. In
that case, the user should insert the device that the user wants to load the
model into.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>folder</strong> (<em>{str}</em>) – folder into which the model should be saved.</li>
<li><strong>name</strong> (<em>{str}</em>) – A name to associate the model to load. It is absolutelty possible to save
a number of models within the same folder, and hence the name can retrieve
that model that is important.</li>
<li><strong>map_location</strong> (<em>{str}</em><em>, </em><em>optional</em>) – The device in which to load the file. This is a string like ‘cpu’, ‘cuad:0’
etc. (the default is None, which results in the model being loaded to the
originam device)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.maxAction">
<code class="descname">maxAction</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.maxAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.maxAction" title="Permalink to this definition">¶</a></dt>
<dd><p>returns the action that maximizes the Q function</p>
<p>Given an set of statees, this function is going to return a set
of actions which will maximize the value of the Q network for each
of the supplied states.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>{nd_array</em><em> or </em><em>tensor}</em>) – numpy array or tensor containing the state. The columns
represent the different parts of the state.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The return values of actions that maximize the states</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">uarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.memoryUpdateEpisode">
<code class="descname">memoryUpdateEpisode</code><span class="sig-paren">(</span><em>policy</em>, <em>maxSteps=1000</em>, <em>minScoreToAdd=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.memoryUpdateEpisode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.memoryUpdateEpisode" title="Permalink to this definition">¶</a></dt>
<dd><p>update the memory</p>
<p>Given a particular policy, this memory is going to take
the policy and generate a series of memories and update
thememory buffer. Generating memories is easier to do
using this function than an external function …</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>policy</strong> (<em>{function}</em>) – This is a function that takes a state and returns an action. This
defines how the agent will explore the environment by changing the
exploration/exploitation scale.</li>
<li><strong>maxSteps</strong> (<em>{number}</em><em>, </em><em>optional</em>) – The maximum number of steps that one shoule have within an episode.
(the default is 1000)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.randomAction">
<code class="descname">randomAction</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.randomAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.randomAction" title="Permalink to this definition">¶</a></dt>
<dd><p>returns a set of random actions for the given states</p>
<p>given the size of the number of actions, this function is going
to return a set of actions that has the same number of actions
as the number of inputs in the shape. For example, if
<code class="docutils literal notranslate"><span class="pre">state.shape</span> <span class="pre">==</span> <span class="pre">(10,</span> <span class="pre">?)</span></code> then the result will be a vector of
size 10. This is in accordance with the redduction in the
dimensionality of the maxAction space.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>{nd_array</em><em> or </em><em>tensor}</em>) – numpy array or tensor containing the state. The columns
represent the different parts of the state.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The return value is set of random actions</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">uarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>folder</em>, <em>name</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.save" title="Permalink to this definition">¶</a></dt>
<dd><p>save the model</p>
<p>This function allows one to save the model, in a folder that is
specified, with the fast and the slow qNetworks, as well as the
memory buffer. Sometimes there may be more than a single agent,
and under those circumstances, the name will come in handy. If the
supplied folder does not exist, it will be generated.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>folder</strong> (<em>{str}</em>) – folder into which the model should be saved.</li>
<li><strong>name</strong> (<em>{str}</em>) – A name to associate the current model with. It is
absolutelty possible to save a number of models within
the same folder.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.sigmaMaxAction">
<code class="descname">sigmaMaxAction</code><span class="sig-paren">(</span><em>state</em>, <em>sigma=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.sigmaMaxAction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.sigmaMaxAction" title="Permalink to this definition">¶</a></dt>
<dd><p>returns the action that maximizes the Q function</p>
<p>Given an set of statees, this function is going to return a set
of actions which will maximize the value of the Q network for each
of the supplied states.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> (<em>{nd_array</em><em> or </em><em>tensor}</em>) – numpy array or tensor containing the state. The columns
represent the different parts of the state.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The return values of actions that maximize the states</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">uarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.softUpdate">
<code class="descname">softUpdate</code><span class="sig-paren">(</span><em>tau=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.softUpdate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.softUpdate" title="Permalink to this definition">¶</a></dt>
<dd><p>update the slow network slightly</p>
<p>This is going to update the slow network slightly. The amount
is dictated by <code class="docutils literal notranslate"><span class="pre">tau</span></code>. This should be a number between 0 and 1.
It will update the <code class="docutils literal notranslate"><span class="pre">tau</span></code> fraction of the slow network weights
with the new weights. This is done for providing stability to the
network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tau</strong> (<em>{number}</em><em>, </em><em>optional</em>) – This parameter determines how much of the fast Networks weights
will be updated to the ne parameters weights (the default is 0.1)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>nSamples=100</em>, <em>sigma=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/Agent_DoubleDQN.html#Agent_DoubleDQN.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.Agent_DoubleDQN.Agent_DoubleDQN.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-lib.agents.policy">
<span id="lib-agents-policy-module"></span><h2>lib.agents.policy module<a class="headerlink" href="#module-lib.agents.policy" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="lib.agents.policy.epsGreedyPolicy">
<em class="property">class </em><code class="descclassname">lib.agents.policy.</code><code class="descname">epsGreedyPolicy</code><span class="sig-paren">(</span><em>agent</em>, <em>randomAgent</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/policy.html#epsGreedyPolicy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.policy.epsGreedyPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="lib.agents.policy.epsGreedyPolicy.act">
<code class="descname">act</code><span class="sig-paren">(</span><em>states</em>, <em>eps</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/policy.html#epsGreedyPolicy.act"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.policy.epsGreedyPolicy.act" title="Permalink to this definition">¶</a></dt>
<dd><p>[summary]</p>
<p>[description]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>{</strong><strong>[</strong><strong>type</strong><strong>]</strong><strong>} --</strong><strong> [</strong><strong>description</strong><strong>]</strong> (<em>eps</em>) – </li>
<li><strong>{</strong><strong>[</strong><strong>type</strong><strong>]</strong><strong>} --</strong><strong> [</strong><strong>description</strong><strong>]</strong> – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">[type] – [description]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-lib.agents.qNetwork">
<span id="lib-agents-qnetwork-module"></span><h2>lib.agents.qNetwork module<a class="headerlink" href="#module-lib.agents.qNetwork" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="lib.agents.qNetwork.qNetworkDiscrete">
<em class="property">class </em><code class="descclassname">lib.agents.qNetwork.</code><code class="descname">qNetworkDiscrete</code><span class="sig-paren">(</span><em>stateSize, actionSize, layers=[10, 5], activations=[&lt;function tanh&gt;, &lt;function tanh&gt;], batchNormalization=False, lr=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/qNetwork.html#qNetworkDiscrete"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.qNetwork.qNetworkDiscrete" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="lib.agents.qNetwork.qNetworkDiscrete.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>sigma=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/qNetwork.html#qNetworkDiscrete.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.qNetwork.qNetworkDiscrete.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>forward function that is called during the forward pass</p>
<p>This is the forward function that will be called during a
forward pass. It takes thee states and gives the Q value
correspondidng to each of the applied actions that are
associated with that state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>{tensor}</em>) – This is a 2D tensor.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">This represents the Q value of the function</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="lib.agents.qNetwork.qNetworkDiscrete.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>v1</em>, <em>v2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/qNetwork.html#qNetworkDiscrete.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.qNetwork.qNetworkDiscrete.step" title="Permalink to this definition">¶</a></dt>
<dd><p>[summary]</p>
<p>[description]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>v1</strong> (<em>{</em><em>[</em><em>type</em><em>]</em><em>}</em>) – [description]</li>
<li><strong>v2</strong> (<em>{</em><em>[</em><em>type</em><em>]</em><em>}</em>) – [description]</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal notranslate"><span class="pre">type</span></code> – [description]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-lib.agents.randomActor">
<span id="lib-agents-randomactor-module"></span><h2>lib.agents.randomActor module<a class="headerlink" href="#module-lib.agents.randomActor" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="lib.agents.randomActor.randomDiscreteActor">
<em class="property">class </em><code class="descclassname">lib.agents.randomActor.</code><code class="descname">randomDiscreteActor</code><span class="sig-paren">(</span><em>stateShape</em>, <em>numActions</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/randomActor.html#randomDiscreteActor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.randomActor.randomDiscreteActor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="lib.agents.randomActor.randomDiscreteActor.act">
<code class="descname">act</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/randomActor.html#randomDiscreteActor.act"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.randomActor.randomDiscreteActor.act" title="Permalink to this definition">¶</a></dt>
<dd><p>return an action based on the state</p>
<p>[description]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>{nd-array} -- nd-array as described by the state</strong> (<em>state</em>) – shape described in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><dl class="docutils">
<dt>integer – integer between 0 and the number of actions</dt>
<dd>available.</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-lib.agents.sequentialActor">
<span id="lib-agents-sequentialactor-module"></span><h2>lib.agents.sequentialActor module<a class="headerlink" href="#module-lib.agents.sequentialActor" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="lib.agents.sequentialActor.SequentialDiscreteActor">
<em class="property">class </em><code class="descclassname">lib.agents.sequentialActor.</code><code class="descname">SequentialDiscreteActor</code><span class="sig-paren">(</span><em>stateSize, numActions, layers=[10, 5], activations=[&lt;function tanh&gt;, &lt;function tanh&gt;], batchNormalization=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/sequentialActor.html#SequentialDiscreteActor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.sequentialActor.SequentialDiscreteActor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="lib.agents.sequentialActor.SequentialDiscreteActor.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/sequentialActor.html#SequentialDiscreteActor.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.sequentialActor.SequentialDiscreteActor.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-lib.agents.sequentialCritic">
<span id="lib-agents-sequentialcritic-module"></span><h2>lib.agents.sequentialCritic module<a class="headerlink" href="#module-lib.agents.sequentialCritic" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="lib.agents.sequentialCritic.SequentialCritic">
<em class="property">class </em><code class="descclassname">lib.agents.sequentialCritic.</code><code class="descname">SequentialCritic</code><span class="sig-paren">(</span><em>stateSize, actionSize, layers=[10, 5], activations=[&lt;function tanh&gt;, &lt;function tanh&gt;], mergeLayer=0, batchNormalization=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/sequentialCritic.html#SequentialCritic"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.sequentialCritic.SequentialCritic" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="lib.agents.sequentialCritic.SequentialCritic.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>action</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/sequentialCritic.html#SequentialCritic.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.sequentialCritic.SequentialCritic.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-lib.agents.trainAgents">
<span id="lib-agents-trainagents-module"></span><h2>lib.agents.trainAgents module<a class="headerlink" href="#module-lib.agents.trainAgents" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="lib.agents.trainAgents.trainAgentGymEpsGreedy">
<code class="descclassname">lib.agents.trainAgents.</code><code class="descname">trainAgentGymEpsGreedy</code><span class="sig-paren">(</span><em>configAgent</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lib/agents/trainAgents.html#trainAgentGymEpsGreedy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#lib.agents.trainAgents.trainAgentGymEpsGreedy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-lib.agents">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-lib.agents" title="Permalink to this headline">¶</a></h2>
<p>module that contains a plethora of agents and policies</p>
<p>This module will contain a number of agents and policies that can be specified
by easily in a uniform manner. This way different agents can be easily swapped
out between each other.</p>
<div class="section" id="agents">
<h3>Agents<a class="headerlink" href="#agents" title="Permalink to this headline">¶</a></h3>
<p>Agents take a state and return an action. Unfortunately, both states and
actions come in a variety of shapes and sizes.</p>
<p>There are essentially two different types of states. Either a vector,
or an nd-array. Typically, nd-arrays are dealt with coonvolution operators
whiile vectors are dealt with simple sequential networks. While nd-arrays
can be flattened, the reverse operator us generally not practicable. In
any case, it is assumed that the user has enough intuition to be able to
distinguish between the two.</p>
<p>Actions are typically vectors. However, sometimes actions can be discrete
and sometimes contnuous and any combination of the two. Furthermore, actions
typically have bounds. In more general cases (like chess) valid actions are
associated with the current state. There is no generic way of solving this
problem, so we chall create different types of agents that will return
different types of actions.</p>
<p>All agets will definitely have the following methods:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">act</span></code>     : action to take given a state</li>
<li><code class="docutils literal notranslate"><span class="pre">save</span></code>    : save the current state of the agent</li>
<li><code class="docutils literal notranslate"><span class="pre">restore</span></code> : restore the agent from a state saved earlier</li>
</ul>
</div></blockquote>
<p>Currently the following agents are available:</p>
</div>
<div class="section" id="policies">
<h3>Policies<a class="headerlink" href="#policies" title="Permalink to this headline">¶</a></h3>
<p>Policies determine what action to take given the result of an agent. In one
way, they are similar in that they also take a state, and return an action.
However, a policy determines how much exploration vs. exploitation should
be done over the period that the agent is learning.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="lib.argParsers.html" class="btn btn-neutral float-right" title="lib.argParsers package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="lib.html" class="btn btn-neutral float-left" title="lib package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Author

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>