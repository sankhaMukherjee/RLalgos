{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sankha/Documents/programs/ML/RLalgos/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. `envGym`\n",
    "\n",
    "`envGym` is a wrapper around the OpenAI Gym environment. It has several methds that can be useful for using the environment. In this notebook, we shall explore this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.envs import envGym\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. The `Env()`  context manager\n",
    "\n",
    "The `Env` class exposes a context manager that will allow this environment to generate an OpenAI environment within. The first time this context manager is entered, this creates a current state `self.state`. This can be reset with the `self.reset()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial environment state:\n",
      " [ 63  63  63  63  63  63 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 192 192 192 192 192 192\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 240   0   0 255   0\n",
      "   0 240   0   5   0   0   6   0  70 182 134 198  22  38  54  70  88   6\n",
      " 146   0   8   0   0   0   0   0   0 241   0 242   0 242  25 241   5 242\n",
      "   0   0 255   0 228   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   8   0 255 255 255 255 255 255 255   0   0   5   0   0 186 214 117 246\n",
      " 219 242]\n",
      "Initial environment state after reset:\n",
      " [ 63  63  63  63  63  63 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 192 192 192 192 192 192\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 240   0   0 255   0\n",
      "   0 240   0   5   0   0   6   0  70 182 134 198  22  38  54  70  88   6\n",
      " 146   0   8   0   0   0   0   0   0 241   0 242   0 242  25 241   5 242\n",
      "   0   0 255   0 228   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   8   0 255 255 255 255 255 255 255   0   0   5   0   0 186 214 117 246\n",
      " 219 242]\n"
     ]
    }
   ],
   "source": [
    "name = 'Breakout-ramNoFrameskip-v4'\n",
    "with envGym.Env(name, showEnv=False) as env:\n",
    "    print(f'Initial environment state:\\n {env.state}')\n",
    "    env.reset()\n",
    "    print(f'Initial environment state after reset:\\n {env.state}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. The `self.env` property\n",
    "\n",
    "The `self.env` property points to the OpenAI Gym environment. Use this for any of the OpenAI Gym methods. However, it is best not to use the internal environment directly. It would be much more preferabe to update this environment to create a new method for this specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check environments action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "name = 'Breakout-ramNoFrameskip-v4'\n",
    "with envGym.Env(name, showEnv=False) as env:\n",
    "    print(f'Check environments action space: {env.env.action_space}')\n",
    "    env.env.render()\n",
    "    sleep(2)\n",
    "    \n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Stepping and playing\n",
    "\n",
    "The `Env` class provides two main methods to interact with the environment - a `self.step(policy)` and an `selef.episode(policy, maxSteps)`. For making this environment compatible with the `envUnity` environment, which can simulate more than a single actor at a single time, this policy is a function that should return a number of actions, one for each actor. For this reason, the result of a policy should always be a list of actions. For the `envGym` environment, this will mean a list with a single action.\n",
    "\n",
    "### 1.3.1. Let us take a couple of steps\n",
    "\n",
    "Note that the _Breakout_ takes a `Discrete(4)` pytorch tensor. Note allso that this returrns a result per actor (only one in this case). We shall specify a randoom action ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking the first step\n",
      "[(array([ 63,  63,  63,  63,  63,  63, 255, 255, 255, 255, 255, 255, 255,\n",
      "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "       255, 255, 255, 255, 192, 192, 192, 192, 192, 192, 255, 255, 255,\n",
      "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 240,   0,   0,\n",
      "       255,   0,   0, 240,   0,   5,   0,   0,   6,   0,  70, 182, 134,\n",
      "       198,  22,  38,  54,  70,  88,   6, 146,   0,   8,   0,   0,   0,\n",
      "         0,   0,   0, 241,   0, 242,   0, 242,  25, 241,   5, 242,   0,\n",
      "         0, 255,   0, 228,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   8,   0, 255, 255, 255, 255, 255, 255, 255,\n",
      "         0,   0,   5,   0,   0, 186, 214, 117, 246, 219, 242], dtype=uint8), 1, 0.0, array([ 63,  63,  63,  63,  63,  63, 255, 255, 255, 255, 255, 255, 255,\n",
      "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "       255, 255, 255, 255, 192, 192, 192, 192, 192, 192, 255, 255, 255,\n",
      "       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 240,   0,   0,\n",
      "       255,   0,   0, 240,   0,   5,   0,   0,   6,   0,  70, 182, 134,\n",
      "       198,  22,  38,  54,  70,  88,   5, 146,   0,   7,   0,   0,   0,\n",
      "         0,   0,   0, 241,   0, 242,   0, 242,  25, 241,   5, 242,   1,\n",
      "         0, 255,   0, 227,  71,   0,   0,   0, 127,   0, 113,   0,   1,\n",
      "         0,   1,   0,   0,   8,   0, 255, 255, 255, 255, 255, 255, 255,\n",
      "         0,   0,   5,   0,   0, 186, 214, 117, 246, 219, 242], dtype=uint8), False)]\n",
      "\n",
      "\n",
      "Taking the second step\n",
      "\n",
      "    state     : \n",
      "[ 63  63  63  63  63  63 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 192 192 192 192 192 192\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 240   0   0 255   0\n",
      "   0 240   0   5   0   0   6   0  70 182 134 198  22  38  54  70  88   5\n",
      " 146   0   7   0   0   0   0   0   0 241   0 242   0 242  25 241   5 242\n",
      "   1   0 255   0 227  71   0   0   0 127   0 113   0   1   0   1   0   0\n",
      "   8   0 255 255 255 255 255 255 255   0   0   5   0   0 186 214 117 246\n",
      " 219 242]\n",
      "    nextState : \n",
      "[ 63  63  63  63  63  63 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 192 192 192 192 192 192\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 240   0   0 255   0\n",
      "   0 240   0   5   0   0   6   0  70 182 134 198  22  38  54  70  94   4\n",
      " 146   0   6   0   0   0   0   0   0 241   0 242   0 242  25 241   5 242\n",
      "   2   0 255   0 226  70   0   0   0 126   0 114   0   1   0   1   0   0\n",
      "   8   0 255 255 255 255 255 255 255   0   0   5   0   0 186 214 117 246\n",
      " 219 242]\n",
      "    action    : 3\n",
      "    done      : False\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "name = 'Breakout-ramNoFrameskip-v4'\n",
    "\n",
    "policy = lambda m: [torch.randint(0, 4, (1,))]\n",
    "with envGym.Env(name, showEnv=False) as env:\n",
    "    \n",
    "    print('Taking the first step')\n",
    "    result = env.step(policy)\n",
    "    print(result)\n",
    "    \n",
    "    print('\\n\\nTaking the second step')\n",
    "    result = env.step(policy)[0]\n",
    "    state, action, reward, nextState, done = result\n",
    "    print(f'''\n",
    "    state     : \\n{state}\n",
    "    nextState : \\n{nextState}\n",
    "    action    : {action}\n",
    "    done      : {done}\n",
    "    ''')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Let us play an entire episode\n",
    "\n",
    "Note that the _Breakout_ takes a `Discrete(4)` pytorch tensor. Note allso that this returrns a result per actor (only one in this case). We shall specify a randoom action ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 0.0, action = 0, done = False\n",
      "reward = 0.0, action = 0, done = False\n",
      "reward = 0.0, action = 1, done = False\n",
      "reward = 0.0, action = 2, done = False\n",
      "reward = 0.0, action = 3, done = False\n",
      "reward = 0.0, action = 3, done = False\n",
      "reward = 0.0, action = 0, done = False\n",
      "reward = 0.0, action = 1, done = False\n",
      "reward = 0.0, action = 2, done = False\n",
      "reward = 0.0, action = 2, done = False\n",
      "final state:\n",
      "[ 63  63  63  63  63  63 255 255 255 255 255 255 255 255 255 255 255 255\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 192 192 192 192 192 192\n",
      " 255 255 255 255 255 255 255 255 255 255 255 255 255 240   0   0 255   0\n",
      "   0 240   0   5   0   0   6   0  70 182 134 198  22  38  54  70  88   3\n",
      " 141   0   6   2   0   0   0   0   0 241   0 242   0 242  25 241   5 242\n",
      "   9   0 255   0 219  65   0   0   0 185   0 119   0   1   0   1   0   0\n",
      "   8   0 255 255 255 255 255 255 255   0   0   4   0   0 186 214 117 246\n",
      " 219 242]\n"
     ]
    }
   ],
   "source": [
    "name = 'Breakout-ramNoFrameskip-v4'\n",
    "\n",
    "policy = lambda m: [torch.randint(0, 4, (1,))]\n",
    "with envGym.Env(name, showEnv=False) as env:\n",
    "    \n",
    "    result = env.episode(policy, 10)[0]\n",
    "    for r in result:\n",
    "        state, action, reward, nextState, done = r\n",
    "        print(f'reward = {reward}, action = {action}, done = {done}')\n",
    "    \n",
    "    print(f'final state:\\n{state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
